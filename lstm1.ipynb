{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was my kernel in the 'Jigsaw unintended bias in Toxicity Classification' Challenge in Kaggle. Here is the link to the competition. \n",
    "[Click Here](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from numpy import array, asarray, zeros\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "train = train[['comment_text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]<>%:')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def text_prepare(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(re.compile('^[a-z][0-9]'), \" \", text)\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE, \" \", text)\n",
    "    text = re.sub(BAD_SYMBOLS_RE, \"\", text)  \n",
    "    querywords = text.split()\n",
    "    resultwords  = [word for word in querywords if word not in STOPWORDS]\n",
    "    text = ' '.join(resultwords)\n",
    "    text = re.sub('\\s+', \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [text_prepare(x) for x in train['comment_text']]\n",
    "X_test = [text_prepare(x) for x in test['comment_text']]\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train+X_test)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_docs_train = t.texts_to_sequences(X_train)\n",
    "encoded_docs_test = t.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = 160\n",
    "padded_docs_train = pad_sequences(encoded_docs_train, maxlen=max_length, padding='post')\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "\n",
    "embeddings_index = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "padded_docs_train, padded_docs_val, en_train, en_val = train_test_split(padded_docs_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "f = open('../input/glove6b300dtxt/glove.6B.300d.txt')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 160, 300)          329914500 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 330,255,501\n",
      "Trainable params: 341,001\n",
      "Non-trainable params: 329,914,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "1624386/1624386 [==============================] - 911s 561us/step - loss: 0.1583 - acc: 0.9427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f363c4e3940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=160, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "model.fit(padded_docs_train, en_train, batch_size=1024, epochs=1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180488/180488 [==============================] - 1280s 7ms/step\n",
      "Accuracy: 94.675546\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs_val, en_val, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "predictions = model.predict(padded_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [i[0] for i in predictions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': prediction\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
